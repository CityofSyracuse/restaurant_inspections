---
title: "Exploratory Analysis on Syracuse Data"
output:
html_document: default
html_notebook: default
---


```{r loadLibraries}
library(data.table) 
library(readr)
library(magrittr)
library(lubridate)
library(ggplot2)
library(plyr)
library(ROCR)
library(knitr)
library(hexbin)
library(RColorBrewer)
library(sp)
library(geosphere)
library(randomForest)
rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
```

```{r readData}
dat <- read_csv("../data/inspections2.csv") %>% data.table()
```

## Examine Variability between inspections and re-inspections

On quick examination, inspector variability appears large. Note that this is 
not necessarily bias, because it could be that inspectors work in better vs. 
worse areas. 

```{r}
dat[ , .(.N,
         mean_critical = mean(nCritical),
         sd_critical = sd(nCritical),
         mean_nonCritical = mean(nNonCritical),
         sd_nonCritical = sd(nNonCritical)), by = inspectionType] %>% 
  kable()

nCritical <- dat[,nCritical]
nNonCritical <- dat[,nNonCritical]
inspectionType <- dat[,inspectionType]

t.test(nCritical[inspectionType=="Inspection"],nCritical[inspectionType=="Re-Inspection"])

ggplot(dat, aes(inspectionType, nCritical)) + 
  geom_boxplot() + labs(title = "Critical Violations by Inspection Type", y = "# Critical Violations", x = "Inspection Type")

t.test(nNonCritical[inspectionType=="Inspection"],nNonCritical[inspectionType=="Re-Inspection"])

ggplot(dat, aes(inspectionType,nNonCritical)) + 
  geom_boxplot() + labs(title = "Non-Critical Violations by Inspection Type", y = "# Non-Critical Violations", x = "Inspection Type")

```

## Histograms of Number of Violations

```{r}

ggplot(dat, aes(nCritical)) + geom_histogram(binwidth = 1, color = "black",fill = "lightblue") + labs(title = "Critical Violations Histogram",x = "Number of Critical Violations",y = "Number of Inspections")

ggplot(dat, aes(nNonCritical)) + geom_histogram(binwidth = 1, color = "black",fill = "lightblue") + labs(title = "Non-Critical Violations Histogram",x = "Number of Non-Critical Violations",y = "Number of Inspections")
```

## Violations by Facility Type 

```{r}
count(dat[,facilityType])
type = sort(unique(dat[,facilityType]))
code <- dat[,ID]
ind = 0
nType = c()
for (i in type) {
    iThisType <- (dat[,facilityType]==i)
    ind = ind+1
    nType[ind] <- length(unique(code[iThisType]))
}

ggplot(dat, aes(facilityType,nCritical)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  labs(title = "Critical Violations by Facility Type",y = "# Critical Violations",x="")

ggplot(dat, aes(facilityType,nNonCritical)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  labs(title = "Non-Critical Violations by Facility Type",y = "# Non-Critical Violations",x="")
```


## Are inspections worse for newer restaurants? 

```{r}
ggplot(dat, aes(daysTilExp)) + geom_histogram(bins = 100, color = "black",fill = "lightblue") + labs(title = "Permit Age at Inspection",x = "Days Until Permit Expires",y = "Number of Inspections") + xlim(0,max(dat[,daysTilExp]))

hexbinplot(nCritical~daysTilExp,data=dat,colramp=rf,main = "Critical Violations vs. Age of Permit", xlab = "Days Until Permit Expires", ylab = "# Critical Violations",aspect=1,xlim=c(-110,max(dat[,daysTilExp])))

ds = dat[,daysTilExp,nNonCritical]
h <- hexbin(ds)
hexbinplot(nNonCritical~daysTilExp,data=dat,colramp=rf,main = "Non-Critical Violations vs. Age of Permit", xlab = "Days Until Permit Expires", ylab = "# Non-Critical Violations",aspect=1,xlim=c(-110,max(dat[,daysTilExp])))

```

## Do restaurants get worse inspections if they haven't been inspected in a while? 

```{r}
dat <- dat[!is.na(nCritical_prev)]

ggplot(dat, aes(daysSincePrev)) + geom_histogram(bins = 100, color = "black",fill = "lightblue") + labs(title = "Inspection Frequency",x = "Days Since Last Inspection",y = "Number of Inspections") + xlim(0,max(dat[,daysSincePrev]))

hexbinplot(nCritical~daysSincePrev,data=dat,colramp=rf,main = "Critical Violations vs. Last Inspection Date", xlab = "Days Since Last Inspection", ylab = "# Critical Violations",aspect=1)

hexbinplot(nCritical~ nCritical_prev,data=dat,colramp=rf,main = "Current and Previous Critical Violations", xlab = "# Critical (Previous)", ylab = "# Critical (Current)",aspect=1)

```

## Quick modeling 

Now we can have an auto-regressive model on previous value. This is also 
known as an AR[1] model. 

Try a binomial model, but based on histogram above, it doesn't appear that 
a cut-off at 1 critical violation is necessarily the best idea. But it is easier
to look at accuracy at least. 

```{r createBinaryVariables, results = "hide"}
dat[ , nCritical_binary := nCritical >= 1]
dat[ , nCritical_prev_binary := nCritical_prev >= 1]

dat_model <- subset(dat, select = c(nCritical_binary,nCritical_prev_binary))
iTrain <- dat[,isTest] == FALSE
iTest <- dat[,isTest] == TRUE

dat_model <- dat_model[complete.cases(dat_model)]  # make complete ONLY for quick modelling

fit <- glm(nCritical_binary ~ nCritical_prev_binary,data = dat_model[iTrain], family = "binomial")

summary(fit)
fitted_values <- predict(fit, dat_model[iTest], type = "response")
pred1 <- prediction(fitted_values, dat_model[iTest]$nCritical_binary)
plot(performance(pred1, "tpr", "fpr"), main="ROC")
performance(pred1, measure = "auc")@y.values  # AUC
```
```{r logisticModel}
dat_model <- subset(dat, select = c(nCritical_binary, 
                                    facilityType,
                                    zip,
                                    nCritical_prev,
                                    nNonCritical_prev,
                                    daysTilExp,
                                    daysSincePrev))
dat_model <- dat_model[complete.cases(dat_model)]  # make complete ONLY for quick modelling
fit <- glm(nCritical_binary ~ .,data = dat_model[iTrain], family = "binomial")
summary(fit)
fitted_values <- predict(fit, dat_model[iTest], type = "response")
pred2 <- prediction(fitted_values, dat_model[iTest]$nCritical_binary)
performance(pred2, measure = "auc")@y.values  # AUC

dat_model <- subset(dat, select = c(nCritical_binary, 
                                    facilityType,
                                    zip,
                                    nCritical_prev,
                                    nNonCritical_prev,
                                    daysTilExp,
                                    daysSincePrev,
                                    avg_neighbor_num_critical,
                                    avg_neighbor_num_non_critical))
dat_model <- dat_model[complete.cases(dat_model)]  # make complete ONLY for quick modelling
fit <- glm(nCritical_binary ~ .,data = dat_model[iTrain], family = "binomial")
summary(fit)
fitted_values <- predict(fit, dat_model[iTest], type = "response")
pred3 <- prediction(fitted_values, dat_model[iTest]$nCritical_binary)
performance(pred3, measure = "auc")@y.values  # AUC

x <- subset(dat_model,select = c(facilityType,
                                    zip,
                                    nCritical_prev,
                                    nNonCritical_prev,
                                    daysTilExp,
                                    daysSincePrev,
                                    avg_neighbor_num_critical,
                                    avg_neighbor_num_non_critical))
y <- subset(dat_model,select = nCritical_binary);
model <- randomForest(x=x,y=y,importance=TRUE)

png("../doc/roc.png")
plot(performance(pred1, "tpr", "fpr"), main="ROC")
plot(performance(pred2, "tpr", "fpr"), main="ROC",add = TRUE,col="blue")
plot(performance(pred3, "tpr", "fpr"), main="ROC",add = TRUE,col="red")
legend(0.4,0.3,legend=c("Baseline (AR-1)","Logistic Regression (Original)","Logistic Regression (New Features Included)"),lty=c(1,1),col=c("black","blue","red"))
```


## TODO
- Incorporate features extracted from additional data sets (liquor licenses, crime, census, complains, weather, yelp reviews)
- Try different types of models (ridge regression, random forest, svm)
- Use cross-validation or define a reasonable test set

## Some questions

# Alex's coments all valid
- Do we want to include all facilities, or just restaurants? 
- Is it ok to use previous observations? 
- Should we threshold? If so, where is best place to threshold? 
- Are there violation codes that are predictive of whether the inspections will go poorly the next time?

# Additional Chris comments
- Should we predit on a per-inspection or a per-facility basis? Chicago was per-inspection but I think per-facility makes more sense.
- Merits of cross-validation vs. test set? Setting up a good cross-validation scheme could be difficult. I would propose using one year of data for test.
- This is essentailly a classification problem, so we should use a classifier (svm or random forest)
- 