---
title: "Exploratory Analysis on Raleigh Data"
output:
html_document: default
html_notebook: default
---


```{r loadLibraries}
suppressMessages(library(data.table))
suppressMessages(library(readr))
suppressMessages(library(magrittr))
suppressMessages(library(lubridate))
suppressMessages(library(ggplot2))
suppressMessages(library(plyr))
suppressMessages(library(ROCR))
suppressMessages(library(knitr))
suppressMessages(library(caret))
```

```{r readData}
dat <- fread("../data/merged.csv")
dat[ , zip := as.character(zip)]
```


## Examine Inspect Variability 

On quick examination, inspector variability appears large. Note that this is 
not necessarily bias, because it could be that inspectors work in better vs. 
worse areas. 

```{r}
dat[ , .(mean_critical = mean(num_critical), .N,
         sd_critical = sd(num_critical)), by = InspectedBy][order(-mean_critical)] %>% 
  kable()
```

## Examine counts of critical vs. non-critical violations. 

How often does an inspection result in a at least one critical violation? 

```{r}
dat[ , mean(num_critical == 0)]
dat[ , mean(num_critical >= 1)]
```

```{r}
dat[ , summary(num_critical)]
ggplot(dat, aes(num_critical)) + geom_histogram(binwidth = 1, color = "black", 
                                                fill = "lightblue") + 
  labs(title = "Histogram of Number of Critical Violations") 
ggplot(dat, aes(num_non_critical)) + geom_histogram(binwidth = 1, color = "black", 
                                                    fill = "lightblue") + 
  labs(title = "Histogram of Number of NON-Critical Violations") 
ggplot(dat, aes(num_critical, Score)) + geom_point() + 
  labs(title = "Score vs. Number of Critical Violations")
ggplot(dat, aes(num_critical + num_non_critical, Score)) + geom_point() +
  labs(title = "Score vs. Number of All (Critical+Non-Critical) Violations")
```

## Violations by Facility Type 

```{r}
ggplot(dat, aes(FacilityType, num_critical)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(title = "Critical Violations by Facility Type")
ggplot(dat, aes(FacilityType, Score)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(title = "Food Inspection Score by Facility Type")
```

## Counts of inspections per restaurant

If we want to do time-series, we need to make sure we have enough observations
per restaurant. 

```{r}
dat[ , .(num_inspections = uniqueN(Date)), by = HSISID] %>% 
  ggplot(aes(num_inspections)) + geom_histogram(binwidth = 1, color = "black", 
                                                fill = "lightblue") + 
  labs(title = "Number of Observations (Inspections) per Restaurant")
```

## Are inspections worse for first inspections or newer restaurants? 

```{r, results = 'hide'}
dat[ , num_inspect := 1:.N, by = HSISID]
dat[ , .(mean_num_critical = mean(num_critical)), by = num_inspect] %>% 
  ggplot(aes(num_inspect, mean_num_critical)) +
  geom_point() + 
  labs(title = "Average Number of Critical Violations By Inspection Visit Number")
```

Let's look at how long restaurants are open and see if that is associated. 

```{r, results = 'hide'}
dat[ , years_from_opening := as.numeric(difftime(Date, RestaurantOpenDate, 
                                                 units = "days")) / 365]
dat[ , .(mean_num_critical = mean(num_critical)), 
     by = .(years_from_opening = round(years_from_opening))] %>% 
  ggplot(aes(years_from_opening, mean_num_critical)) +
  geom_point() + 
  labs(title = "Average Number of Critical Violations By Year From Opening")
```

## Do restaurants get worse inspections if they haven't been inspected in a while? 

TODO (Alex): change to days since last inspection; currently does days since first inspection. 

```{r, results = 'hide'}
dat <- dat[order(HSISID, Date)]
dat[ , days_since_first_inspection := as.numeric(difftime(Date, Date[1], units = "days")), 
     by = HSISID] 
dat[ , .(mean_num_critical = mean(num_critical)), 
     by = .(days_since_first_inspection = round_any(days_since_first_inspection, 10))] %>% 
  ggplot(aes(days_since_first_inspection, mean_num_critical)) + 
  geom_point() + 
  geom_line() + 
  labs(title = "Mean Number of Critical Violations by Days Since First Inspection
(Days are binned by rounding to nearest 10 days)")
```

## Census data 

```{r cor, echo = FALSE}
income_cols <- c("Median_family_income_dollars", 
                 "Median_household_income_dollars", 
                 "Per_capita_income_dollars", 
                 "Percent_Families_Below_Poverty_Line", 
                 "Percent_Food_Stamp/SNAP_benefits_in_the_past_12_months", 
                 "Percent_Supplemental_Security_Income")
dat_income <- dat[ , lapply(.SD, unique), by = zip, .SDcols = income_cols]
dat_income <- dat_income[ , !"zip", with = FALSE]
kable(cor(dat_income))
```

## Quick modeling 

Now we can have an auto-regressive model on previous value. This is also 
known as an AR[1] model. 

```{r}
fit <- glm(num_critical ~ num_critical_previous, 
           data = dat, family = "poisson")
summary(fit)
```



Try a binomial model, but based on histogram above, it doesn't appear that 
a cut-off at 1 critical violation is necessarily the best idea. But it is easier
to look at accuracy at least. 

```{r createBinaryVariables, results = "hide"}
dat[ , num_critical_binary := num_critical >= 1]
dat[ , num_critical_previous_binary := num_critical_previous >= 1]
```



```{r prepareDataForModel, results = "hide"}
dat[ , num_critical_binary := factor(num_critical >= 1)]
yelp_cats <- c("hotdogs", "sandwiches", "pizza", "tradamerican", "burgers", 
               "mexican", "grocery", "breakfast_brunch", "coffee", "chinese", 
               "italian", "newamerican", "chicken_wings", "delis", "bars", 
               "salad", "seafood", "bbq", "bakeries", "sushi")
dat_model <- subset(dat, select = c("num_critical_binary", "Date",
                                    "num_critical_previous",
                                    "days_from_open_date",
                                    "Median_household_income_dollars",
                                    "Percent_Families_Below_Poverty_Line", 
                                    "avg_neighbor_num_critical", 
                                    "rating", "price", yelp_cats  # yelp
))
```

#normalizing data prior to placement in model (Alicia)
```{r normalizingData}
 dat_model
 cat_vars <- c(yelp_cats, "price", "rating", "num_critical_binary", "Date")
 cat_dat <- dat_model[,cat_vars, with=FALSE]
 reg_dat <- dat_model[,!cat_vars, with=FALSE]
 dat_model <- cbind(cat_dat, data.table(scale(reg_dat)))
 dat_model
```


```{r splittingTrainAndTestData, result = "hide"}
dat_model <- dat_model[complete.cases(dat_model)]  # make complete ONLY for quick modelling
dat_model[ , price := factor(price, levels = c("$", "$$", "$$$", "$$$$"))]

# Make train/test splits. 
train <- dat_model[Date < as.POSIXct("2016-01-01")]
test <- dat_model[Date >= as.POSIXct("2016-01-01")]
train[ , Date := NULL]
test[ , Date := NULL]
```

```{r logisticModelUsingGlmnet}
suppressMessages(library(glmnet))

y <- train$num_critical_binary
x <- train
x$num_critical_binary <- NULL
x <- as.matrix(x)
#remove num_critical_binaryTrue and num_critical_binaryFalse
fit = glmnet(x = x, y = y, family = "binomial")
summary(fit)
fitted_values <- predict(fit, newdata = test, type="response")
```

```{r logisticModel}
# Fit model. 
fit <- glm(num_critical_binary ~ ., data = train, family = "binomial")
summary(fit)
fitted_values <- predict(fit, newdata = test, type = "response")
```

Assess fit of logistic model. 

```{r logisticFit}
pred <- prediction(fitted_values, test$num_critical_binary)

# ROC.
plot(performance(pred, "tpr", "fpr"), main="ROC")
abline(0, 1, lty=2)

## sensitivity / specificity
plot(performance(pred, "sens", "spec"), main="sensitivity vs specificity")
abline(1, -1, lty=2)

## phi
plot(performance(pred, "phi"), main="phi scores")

## Fancy ROC curve:
op <- par(bg="lightgray", mai=c(1.2,1.5,1,1))
plot(performance(pred,"tpr","fpr"), 
     main="ROC Curve", colorize=TRUE, lwd=10)
par(op)

## Effect of using a cost function on cutoffs
plot(performance(pred, "cost", cost.fp = 1, cost.fn = 1), 
     main="Even costs (FP=1 TN=1)")
plot(performance(pred, "cost", cost.fp = 1, cost.fn = 4), 
     main="Higher cost for FN (FP=1 TN=4)")

## Accuracy
plot(performance(pred, measure = "acc"))

# AUC.
performance(pred, measure = "auc")@y.values[[1]]  # AUC
```

```{r confusionMatrix}
confusionMatrix(fitted_values > 0.5, test$num_critical_binary)
```




## Some questions

- Do we want to include all facilities, or just restaurants? 
- Is it ok to use previous observations? 
- Should we threshold? If so, where is best place to threshold? 
- Are there violation codes that are predictive of whether the inspections will go poorly the next time?
- Should we use AUC as main metric? How to do train/test split? I'm partial to using say last 1 year 
or 6 months of data as test set. 
- What was Chicago's AUC _without_ inspector id? 

