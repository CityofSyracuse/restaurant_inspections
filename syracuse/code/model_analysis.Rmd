---
title: "Exploratory Analysis on Syracuse Data"
output:
html_document: default
html_notebook: default
---


```{r loadLibraries}
library(data.table) 
library(readr)
library(magrittr)
library(lubridate)
library(ggplot2)
library(plyr)
library(ROCR)
library(knitr)
library(hexbin)
library(RColorBrewer)
library(sp)
library(geosphere)
library(randomForest)
library(glmnet)
rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
```

```{r readData}
dat <- read_csv("../data/inspections2.csv") %>% data.table()
```

## Examine Variability between inspections and re-inspections

On quick examination, inspector variability appears large. Note that this is 
not necessarily bias, because it could be that inspectors work in better vs. 
worse areas. 

```{r}
dat[ , .(.N,
         mean_critical = mean(nCritical),
         sd_critical = sd(nCritical),
         mean_nonCritical = mean(nNonCritical),
         sd_nonCritical = sd(nNonCritical)), by = inspectionType] %>% 
  kable()

nCritical <- dat[,nCritical]
nNonCritical <- dat[,nNonCritical]
inspectionType <- dat[,inspectionType]

t.test(nCritical[inspectionType=="Inspection"],nCritical[inspectionType=="Re-Inspection"])

ggplot(dat, aes(inspectionType, nCritical)) + 
  geom_boxplot() + labs(title = "Critical Violations by Inspection Type", y = "# Critical Violations", x = "Inspection Type")
ggsave("criticalViolations_byInspectionType.jpg")

t.test(nNonCritical[inspectionType=="Inspection"],nNonCritical[inspectionType=="Re-Inspection"])

ggplot(dat, aes(inspectionType,nNonCritical)) + 
  geom_boxplot() + labs(title = "Non-Critical Violations by Inspection Type", y = "# Non-Critical Violations", x = "Inspection Type")
ggsave("nonCriticalViolations_byInspectionType.jpg")

```

## Histograms of Number of Violations

```{r}

ggplot(dat, aes(nCritical)) + geom_histogram(binwidth = 1, color = "black",fill = "lightblue") + labs(title = "Critical Violations Histogram",x = "Number of Critical Violations",y = "Number of Inspections")
ggsave("criticalViolations.jpg")

ggplot(dat, aes(nNonCritical)) + geom_histogram(binwidth = 1, color = "black",fill = "lightblue") + labs(title = "Non-Critical Violations Histogram",x = "Number of Non-Critical Violations",y = "Number of Inspections")
ggsave("nonCriticalViolations.jpg")
```

## Violations by Facility Type 

```{r}
isRegInspection <- dat[,inspectionType]=="Inspection"
type = dat[isRegInspection,facilityType]
uType = sort(unique(type))
code <- dat[isRegInspection,ID]
ind = 0
nType = c()
for (i in uType) {
    ind = ind+1
    nType[ind] <- length(unique(code[type==i]))
}

ggplot(dat[isRegInspection], aes(facilityType,nCritical)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  labs(title = "Critical Violations by Facility Type",y = "# Critical Violations",x="")
ggsave("nCritical_byFacilityType_regInspectionsOnly.jpg")

ggplot(dat[isRegInspection], aes(facilityType,nNonCritical)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
  labs(title = "Non-Critical Violations by Facility Type",y = "# Non-Critical Violations",x="")
ggsave("nNonCritical_byFacilityType_regInspectionsOnly.jpg")
```

## Quick modeling 

Now we can have an auto-regressive model on previous value. This is also 
known as an AR[1] model. 

Try a binomial model, but based on histogram above, it doesn't appear that 
a cut-off at 1 critical violation is necessarily the best idea. But it is easier
to look at accuracy at least. 

```{r logisticModel}

iTrain = dat[,isTest]==FALSE
iTest = dat[,isTest]==TRUE
isRestaurant = dat[,facilityType]=="Restaurant"
dat[ , nCritical_binary := nCritical >= 1]
cols <- 'zip'
dat[,(cols):=lapply(.SD, as.factor),.SDcols=cols] # convert zip to factor

aucs = matrix(data=NA,ncol=6,nrow=2)

experiments = c(1,2)
for (i in experiments) {
if (i==1) {
dat_train = dat[iTrain & isRegInspection & isRestaurant]
dat_test = dat[iTest & isRegInspection & isRestaurant]
} else {
dat_train = dat[iTrain & isRegInspection]
dat_test = dat[iTest & isRegInspection]
}

X_train <- model.matrix(~nCritical_binary + alcLicense + zip + nCritical_prev + nNonCritical_prev + daysTilExp + daysSincePrev + avg_neighbor_num_critical + avg_neighbor_num_non_critical, dat_train)
Y_train <- X_train[,2]
X_train <- X_train[,-c(1,2)]

X_test <- model.matrix(~nCritical_binary + alcLicense + zip + nCritical_prev + nNonCritical_prev + daysTilExp + daysSincePrev + avg_neighbor_num_critical + avg_neighbor_num_non_critical, dat_test)
Y_test <- X_test[,2]
X_test <- X_test[,-c(1,2)]

# Baseline - predict from number of previous violations
fit <- glmnet(X_train[,c(21,22)],Y_train,alpha=0,lambda=0)
Yhat_test <- predict(fit,X_test[,c(21,22)],type="link")
pred <- prediction(Yhat_test, Y_test)
perf = performance(pred,"auc")@y.values
aucs[i,1] <- as.numeric(perf)
if (i==1){
    jpeg("roc_restaurantsOnly.jpg")
plot(performance(pred, "tpr", "fpr"), main="ROC (Regular Inspections, Restaurants Only)", lty = 2)
} else {
    jpeg("roc_allFacilities.jpg")
plot(performance(pred, "tpr", "fpr"), main="ROC (Regular Inspections, All Facilities)", lty = 2)
}

# Random Forest
fit_RF <- randomForest(X_train,as.factor(Y_train),importance = TRUE)
Yhat_test <- predict(fit_RF, X_test, type = "prob")
Yhat_test <- Yhat_test[,-1]
pred <- prediction(Yhat_test, Y_test)
perf = performance(pred,"auc")@y.values
aucs[i,2] <- as.numeric(perf)
plot(performance(pred, "tpr", "fpr"), add=TRUE)

# Logistic with no regularization
fit <- glmnet(X_train,Y_train,alpha=0,lambda=0)
Yhat_test <- predict(fit,X_test,type="link")
pred <- prediction(Yhat_test, Y_test)
perf = performance(pred,"auc")@y.values
aucs[i,3] = as.numeric(perf)
plot(performance(pred, "tpr", "fpr"), col = "green", add=TRUE)
# fit <- glmnet(X_train,Y_train,alpha=0.5,lambda=fit$lambda[bestInd])
# coef = coefficients(fit)

# 
# Logistic with l1 regularization only
fit <- glmnet(X_train,Y_train,alpha=1)
Yhat_test <- predict(fit,X_test,type="link")
pred <- prediction(Yhat_test, matrix(Y_test,nrow=nrow(Yhat_test),
                                     ncol=ncol(Yhat_test)))
perf = performance(pred,"auc")@y.values
bestInd = which.max(unlist(perf))
aucs[i,4] = as.numeric(perf[bestInd])
pred <- prediction(Yhat_test[,bestInd], Y_test)
plot(performance(pred, "tpr", "fpr"), add = TRUE, col = "red")
#fit <- glmnet(X_train,Y_train,alpha=0.5,lambda=fit$lambda[bestInd])
#coef = coefficients(fit)

# Logistic with l2 regularization only
fit <- glmnet(X_train,Y_train,alpha=0)
Yhat_test <- predict(fit,X_test,type="link")
pred <- prediction(Yhat_test, matrix(Y_test,nrow=nrow(Yhat_test),
                                     ncol=ncol(Yhat_test)))
perf = performance(pred,"auc")@y.values
bestInd = which.max(unlist(perf))
aucs[i,5] = as.numeric(perf[bestInd])
pred <- prediction(Yhat_test[,bestInd], Y_test)
plot(performance(pred, "tpr", "fpr"), add = TRUE, col = "blue")
fit <- glmnet(X_train,Y_train,alpha=0,lambda=fit$lambda[bestInd])
coef = coefficients(fit)

# Logistic with both l1 and l2
fit <- glmnet(X_train,Y_train,alpha=0.5)
Yhat_test <- predict(fit,X_test,type="link")
pred <- prediction(Yhat_test, matrix(Y_test,nrow=nrow(Yhat_test),
                                     ncol=ncol(Yhat_test)))
perf = performance(pred,"auc")@y.values
bestInd = which.max(unlist(perf))
aucs[i,6] = as.numeric(perf[bestInd])
pred <- prediction(Yhat_test[,bestInd], Y_test)
plot(performance(pred, "tpr", "fpr"), col = "violet", add = TRUE)
fit <- glmnet(X_train,Y_train,alpha=0.5,lambda=fit$lambda[bestInd])
coef = coefficients(fit)

if (i==1) {
    legend(0.6,0.6,c('Baseline','Random Forest','Logistic, No Reg','Logistic, L1 Reg','Logistic, L2 Reg','Logistic, L1+L2 Reg'),col=c('black','black','green','red','blue','violet'),lty=c(2,1,1,1,1,1))
    dev.off()
    jpeg("varImportance_restaurantsOnly.jpg")
varImpPlot(fit_RF, main = "RF Variable Importance (Regular Inspections, Restaurants Only)", cex = 0.75)
    dev.off()
} else {
    legend(0.6,0.6,c('Baseline','Random Forest','Logistic, No Reg','Logistic, L1 Reg','Logistic, L2 Reg','Logistic, L1+L2 Reg'),col=c('black','black','green','red','blue','violet'),lty=c(2,1,1,1,1,1))
    dev.off()
    jpeg("varImportance_allFacilities.jpg")
varImpPlot(fit_RF, main = "RF Variable Importance (Regular Inspections, All Facilities)", cex = 0.75)
    dev.off()
}

}
```

## TODO
- Incorporate features extracted from additional data sets (liquor licenses, crime, census, complains, weather, yelp reviews)
- Try different types of models (ridge regression, random forest, svm)
- Use cross-validation or define a reasonable test set

## Some questions

# Alex's coments all valid
- Do we want to include all facilities, or just restaurants? 
- Is it ok to use previous observations? 
- Should we threshold? If so, where is best place to threshold? 
- Are there violation codes that are predictive of whether the inspections will go poorly the next time?

# Additional Chris comments
- Should we predit on a per-inspection or a per-facility basis? Chicago was per-inspection but I think per-facility makes more sense.
- Merits of cross-validation vs. test set? Setting up a good cross-validation scheme could be difficult. I would propose using one year of data for test.
- This is essentailly a classification problem, so we should use a classifier (svm or random forest)
- 